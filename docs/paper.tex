% *==================================================================================*
% *                     Review vs. Camera-Ready settings                             *
% *==================================================================================*
%
% REVIEW: Use the following command for submitting the paper (double-blind,
% for review):
% \documentclass{Interspeech}
%
% CAMERA-READY: Use the following command for the camera-ready version, one
% affiliation per line:
%cameraready
\documentclass[double-blind]{Interspeech}
% *==================================================================================*


% **************************************
% *                                    *
% *      STOP !   DO NOT DELETE !      *
% *          READ THIS FIRST           *
% *                                    *
% * This template also includes        *
% * important INSTRUCTIONS that you    *
% * must follow when preparing your    *
% * paper. Read it BEFORE replacing    *
% * the content with your own work.    *
% **************************************

%==================================================================================
% Title
% Must exactly match the title entered into the paper submission system
\title{Noise-Conditioned Diffusion for Robust Speech Enhancement with Classifier-Free Guidance}

%==================================================================================
% Authors
% The order of authors here must exactly match the order entered into the paper submission system
% Note that the COMPLETE list of authors MUST be entered into the paper submission system at the outset, including when submitting your manuscript for double-blind review
% The ORCID number is still optional but will become mandatory in the future years. It is strongly encouraged to get an ORCID for each cu-author.
% Middle names, including initials, must be included in the first name
\author[affiliation={1}, orcid=0000-0000-0000-0000]{FirstNameA}{LastNameA}
\author[affiliation={1}, correspondingauthor]{FirstNameB}{LastNameB}
% The maximum number of authors in the author list is 20. If the number of contributing authors is more than this, they should be listed in a footnote or the acknowledgement section.

%==================================================================================
% Affiliations

\address{
    $^1$ Korea Advanced Institute of Science and Technology (KAIST), South Korea
}

%==================================================================================
% Emails
\email{author1@kaist.ac.kr, author2@kaist.ac.kr}

%==================================================================================
% Keywords
\keywords{Speech Enhancement, Diffusion Models, Classifier-Free Guidance, Noise Conditioning, Out-of-Distribution Generalization}

\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\usepackage{comment}
\usepackage{kotex}
\usepackage{amsmath, amssymb, amsfonts, bm}
\usepackage{booktabs}  % for \toprule, \midrule, \bottomrule
\usepackage{algorithm}
\usepackage{algpseudocode}
%==================================================================================
% Content

\begin{document}

\maketitle

% the abstract here must exactly match the abstract entered into the paper submission system
\begin{abstract}
% TODO: 실험 완료 후 작성
\end{abstract}


%==================================================================================
% Section 1: Introduction
%==================================================================================
\section{Introduction}

% Para 1: Background - Speech enhancement 중요성
Speech enhancement aims to recover clean speech from noisy observations and plays a crucial role in various applications including telecommunications, hearing aids, and automatic speech recognition systems.
With the increasing prevalence of remote communication, the demand for robust speech enhancement in diverse acoustic environments has grown significantly.

% Para 2: Diffusion 기반 SE의 발전
Recent advances in diffusion-based generative models have shown promising results for speech enhancement~\cite{richter2023speech, lu2022conditional}.
SGMSE+~\cite{richter2023speech} learns to model the conditional distribution of clean speech given noisy observations through score-based generative modeling in the complex STFT domain, achieving state-of-the-art performance on benchmark datasets.
However, most existing approaches are trained on limited noise types and often fail to generalize when encountering unseen noise environments at test time.

% Para 3: 기존 noise-aware 방법들과 한계
Recent works have explored noise-aware approaches to improve generalization.
NASE~\cite{hu2023nase} introduces a noise classification model to produce acoustic embeddings as a conditioner for diffusion-based SE, while NADiffuSE~\cite{wang2023nadiffuse} extracts noise representations as global conditional information.
However, these methods rely on accurate noise characterization during inference---when the noise type is misclassified or the noise embedding is out-of-distribution, the conditioning can mislead the enhancement process rather than help it.

% Para 4: 제안하는 방법 개요 - CFG의 장점
In this work, we propose a noise-conditioned speech enhancement framework that leverages Classifier-Free Guidance (CFG)~\cite{ho2022classifier} to address this limitation.
Unlike prior noise-aware methods, CFG trains the model with conditional dropout, enabling it to perform both conditional and unconditional denoising.
This allows the model to gracefully fall back to unconditional enhancement when the noise conditioning is unreliable, providing robustness against OOD noise types without sacrificing in-distribution performance.

% Para 5: 주요 결과 요약 (TODO: 실험 결과 숫자 업데이트)
Extensive experiments demonstrate that our approach achieves superior OOD generalization.
On ESC-50~\cite{piczak2015esc} noise at 0 dB SNR, our method improves SI-SDR compared to the unconditional baseline.
Furthermore, we provide systematic analysis of noise encoder architectures and CFG hyperparameters to guide practical deployment.

% Contributions
Our main contributions are summarized as follows:
\begin{itemize}
\item \textbf{Noise-Conditional Diffusion Framework:}
We integrate Classifier-Free Guidance (CFG)~\cite{ho2022classifier} into diffusion-based speech enhancement, enabling adaptive noise conditioning that improves OOD generalization.

\item \textbf{Noise Reference Guidance (NRG):}
We introduce an inference-time strategy that leverages short noise reference samples to guide the enhancement process, achieving robustness improvements on unseen noise types.

\item \textbf{Comprehensive Encoder Analysis:}
We compare noise encoder architectures including CNN-based encoders and pre-trained CLAP~\cite{wu2023large} encoders, demonstrating that optimal CFG dropout rates depend on encoder characteristics.
\end{itemize}


%==================================================================================
% Section 2: Related Work (Optional)
%==================================================================================
% \section{Related Work}
% TODO: 필요시 작성
% - Diffusion-based speech enhancement (SGMSE, CDiffuSE, etc.)
% - Noise-aware/adaptive methods
% - Classifier-Free Guidance in audio


%==================================================================================
% Section 3: Methodology
%==================================================================================
\section{Methodology}

% TODO: Figure 1 - Overall architecture diagram

\subsection{Preliminaries: Score-Based Speech Enhancement}
We build upon SGMSE+~\cite{richter2023speech}, which models the conditional distribution $p(x|y)$ of clean speech $x$ given noisy observation $y$ using score-based generative modeling.
The forward diffusion process is defined by the SDE:
\begin{equation}
\mathrm{d}x_t = \gamma (y - x_t)\,\mathrm{d}t + g(t)\,\mathrm{d}w_t
\end{equation}
where $\gamma$ controls the mean reversion towards $y$, and $g(t)$ is the diffusion coefficient.
The score network $s_\theta(x_t, y, t)$ is trained to approximate $\nabla_{x_t} \log p_t(x_t | y)$, and clean speech is generated by solving the reverse SDE.

\subsection{Noise-Conditioned Score Network}
We extend the score network to additionally condition on a noise embedding $z_r = E_\phi(r)$, where $r$ is a short noise reference signal:
\begin{equation}
s_\theta(x_t, y, z_r, t) \approx \nabla_{x_t} \log p_t(x_t \mid y, z_r)
\end{equation}

The noise encoder $E_\phi$ is a 4-layer CNN that processes the noise reference spectrogram and produces a 512-dimensional embedding.
This embedding is added to the diffusion timestep embedding and injected into the score network via FiLM (Feature-wise Linear Modulation)~\cite{perez2018film} layers.

\subsection{Classifier-Free Guidance for Noise Conditioning}
To enable robust conditioning that gracefully degrades for OOD noise, we adopt Classifier-Free Guidance (CFG)~\cite{ho2022classifier}.
During training, we randomly drop the noise conditioning with probability $p$:
\begin{equation}
z_r' =
\begin{cases}
\mathbf{0} & \text{with probability } p \\
z_r & \text{otherwise}
\end{cases}
\end{equation}

At inference time, we compute both conditional and unconditional scores and combine them with guidance scale $w$:
\begin{equation}
\tilde{s} = s_\theta(x_t, y, \mathbf{0}, t) + w \cdot \left( s_\theta(x_t, y, z_r, t) - s_\theta(x_t, y, \mathbf{0}, t) \right)
\end{equation}

This formulation allows the model to leverage noise conditioning when reliable while falling back to unconditional enhancement when the noise embedding is out-of-distribution.


%==================================================================================
% Section 4: Experiments
%==================================================================================
\section{Experiments}

\subsection{Experimental Setup}

\subsubsection{Datasets}
We use VoiceBank-DEMAND~\cite{valentini2016investigating} for training and in-distribution evaluation, which contains 28 speakers with 10 noise types from the DEMAND database~\cite{thiemann2013demand} at SNRs of 0, 5, 10, and 15 dB.
For out-of-distribution evaluation, we create test mixtures using clean speech from VoiceBank with ESC-50~\cite{piczak2015esc} environmental sounds at 0 dB SNR.

\subsubsection{Evaluation Metrics}
We report PESQ~\cite{rix2001perceptual} (Perceptual Evaluation of Speech Quality), ESTOI~\cite{jensen2016algorithm} (Extended Short-Time Objective Intelligibility), and SI-SDR~\cite{le2019sdr} (Scale-Invariant Signal-to-Distortion Ratio).

\subsubsection{Implementation Details}
We build upon SGMSE+~\cite{richter2023speech} with the NCSN++~\cite{song2020score} backbone.
The noise encoder is a 4-layer CNN followed by adaptive pooling and two fully-connected layers, outputting a 512-dimensional embedding.
Models are trained using Adam optimizer with learning rate $10^{-4}$.
We use 50 reverse diffusion steps with the PC sampler for enhancement.
% TODO: 최종 학습 설정으로 업데이트


%==================================================================================
% Section 5: Results
%==================================================================================
\section{Results}

% TODO: 최종 실험 결과로 업데이트 필요
\subsection{Main Results}
Table~\ref{tab:main_results} shows speech enhancement performance on in-distribution (VoiceBank-DEMAND) and out-of-distribution (ESC-50 noise) test sets.

\begin{table}[t]
\centering
\caption{Speech enhancement performance. In-dist: VoiceBank-DEMAND. OOD: ESC-50 noise at 0 dB SNR. SDR denotes SI-SDR.}
\label{tab:main_results}
\small
\begin{tabular}{l|ccc|ccc}
\toprule
& \multicolumn{3}{c|}{In-dist} & \multicolumn{3}{c}{OOD} \\
Method & PESQ & ESTOI & SDR & PESQ & ESTOI & SDR \\
\midrule
SGMSE+~\cite{richter2023speech} & - & - & - & - & - & - \\
Ours (NC+CFG) & - & - & - & - & - & - \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation: CFG Dropout Rate}
We investigate the effect of the conditional dropout probability $p$.
Table~\ref{tab:ablation_p} shows that the optimal $p$ balances between in-distribution performance and OOD generalization.

\begin{table}[t]
\centering
\caption{Effect of CFG dropout probability $p$.}
\label{tab:ablation_p}
\small
\begin{tabular}{c|cc|cc}
\toprule
& \multicolumn{2}{c|}{In-dist} & \multicolumn{2}{c}{OOD} \\
$p$ & PESQ & SDR & PESQ & SDR \\
\midrule
0.0 & - & - & - & - \\
0.1 & - & - & - & - \\
0.2 & - & - & - & - \\
0.3 & - & - & - & - \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation: Noise Encoder Architecture}
We compare CNN-based encoder trained from scratch with pre-trained CLAP~\cite{wu2023large} encoder.
% TODO: 결과 분석


%==================================================================================
% Section 6: Conclusion
%==================================================================================
\section{Conclusion}
% TODO: 실험 완료 후 작성


\section{Generative AI Use Disclosure}
Generative AI tools (Claude) were used for code development assistance and manuscript editing.
All experimental design, analysis, and scientific contributions are the work of the authors.






\bibliographystyle{IEEEtran}
\bibliography{mybib}

\end{document}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: t
%%% End:
