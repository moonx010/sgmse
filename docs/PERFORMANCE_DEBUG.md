# Paper Improvement & Debugging Tracker

> ì´ ë¬¸ì„œëŠ” Interspeech 2026 ë…¼ë¬¸ ì œì¶œì„ ìœ„í•´ í•´ê²°í•´ì•¼ í•  ê¸°ìˆ ì  ë¬¸ì œì™€ ë…¼ë¬¸ í’ˆì§ˆ ê°œì„  ì‚¬í•­ì„ ì¶”ì í•©ë‹ˆë‹¤.

---

## 0. Reviewer Perspective Analysis (Interspeech ê¸°ì¤€)

### 0.1 ì˜ˆìƒ Rejection ì‚¬ìœ 

#### âŒ Critical Issues (Accept ë¶ˆê°€)

| Issue | Severity | í˜„ì¬ ìƒíƒœ | í•´ê²° ë°©ì•ˆ |
|-------|----------|----------|----------|
| **ì‹¤í—˜ ê²°ê³¼ê°€ ì£¼ì¥ì„ ë’·ë°›ì¹¨í•˜ì§€ ì•ŠìŒ** | Critical | CFGê°€ OODì—ì„œ ì˜¤íˆë ¤ ë‚˜ì¨ | ì›ì¸ íŒŒì•… ë° ì¬ì‹¤í—˜ í•„ìˆ˜ |
| **Baseline ì¬í˜„ ì‹¤íŒ¨** | Critical | PESQ 2.9 vs 1.88 | Eval ì„¤ì • ê²€ì¦ í•„ìš” |
| **Novelty ë¶€ì¡±** | Major | CFGë¥¼ SEì— ì ìš©ë§Œ í•¨ | ì¶”ê°€ contribution í•„ìš” |

#### âš ï¸ Major Issues (Major Revision)

| Issue | Description | í•´ê²° ë°©ì•ˆ |
|-------|-------------|----------|
| **ë¹„êµ ì‹¤í—˜ ë¶€ì¡±** | ë‹¤ë¥¸ noise-aware SE ë°©ë²•ê³¼ ë¹„êµ ì—†ìŒ | MetricGAN+, DEMUCS ë“±ê³¼ ë¹„êµ |
| **OOD ë°ì´í„°ì…‹ ë‹¨ì¼** | ESC-50ë§Œ ì‚¬ìš© | UrbanSound8K, AudioSet ì¶”ê°€ |
| **Analysis ë¶€ì¡±** | Noise encoderê°€ ë­˜ í•™ìŠµí•˜ëŠ”ì§€ ë¶„ì„ ì—†ìŒ | t-SNE, attention map ì‹œê°í™” |
| **Ablation ë¶ˆì¶©ë¶„** | Reference length, guidance scale ë“± | ì²´ê³„ì  ablation ì¶”ê°€ |

#### ğŸ“ Minor Issues (Minor Revision)

| Issue | Description |
|-------|-------------|
| Abstract/Conclusion ë¯¸ì‘ì„± | ì‹¤í—˜ ì™„ë£Œ í›„ ì‘ì„± í•„ìš” |
| Related Work ì„¹ì…˜ ì—†ìŒ | í•„ìš”ì‹œ ì¶”ê°€ |
| Figure ë¶€ì¬ | Architecture diagram í•„ìš” |

---

### 0.2 ë…¼ë¬¸ ê°•í™”ë¥¼ ìœ„í•œ í•„ìˆ˜ ì‹¤í—˜

#### A. Baseline ê²€ì¦ (ìµœìš°ì„ )
```
ëª©í‘œ: ìš°ë¦¬ evaluationì´ ì •í™•í•œì§€ í™•ì¸
ë°©ë²•: ë…¼ë¬¸ pretrained checkpointë¡œ ë™ì¼ ê²°ê³¼ ì¬í˜„
ê¸°ëŒ€: PESQ ~2.9, SI-SDR ~17
```

#### B. Conditioning íš¨ê³¼ ì¦ëª…
```
ëª©í‘œ: Noise conditioningì´ ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ”ì§€ ì¦ëª…
ì‹¤í—˜:
1. Zero embedding vs Real embedding ë¹„êµ
2. Random noise reference vs Oracle reference ë¹„êµ
3. Mismatched noise reference í…ŒìŠ¤íŠ¸ (ë‹¤ë¥¸ noise typeìœ¼ë¡œ conditioning)
ê¸°ëŒ€: Real > Zero, Oracle > Random, Matched > Mismatched
```

#### C. OOD ì¼ë°˜í™” ì¦ëª…
```
ëª©í‘œ: ë‹¤ì–‘í•œ OOD í™˜ê²½ì—ì„œ ê°œì„  í™•ì¸
ì‹¤í—˜:
1. ESC-50 (í˜„ì¬)
2. UrbanSound8K
3. AudioSet subset
4. Real-world recordings
ê¸°ëŒ€: ëª¨ë“  OODì—ì„œ baseline ëŒ€ë¹„ ê°œì„ 
```

#### D. ë¹„êµ ì‹¤í—˜
```
ëª©í‘œ: ë‹¤ë¥¸ ë°©ë²•ë“¤ê³¼ ê³µì •í•œ ë¹„êµ
ë¹„êµ ëŒ€ìƒ:
1. SGMSE+ (baseline)
2. MetricGAN+ (discriminative)
3. DEMUCS (end-to-end)
4. CDiffuSE (ë‹¤ë¥¸ diffusion SE)
```

#### E. Analysis & Visualization
```
ëª©í‘œ: ë…¼ë¬¸ì˜ ì„¤ë“ë ¥ ê°•í™”
ì‹¤í—˜:
1. Noise embedding t-SNE (noise typeë³„ clustering)
2. Conditional vs Unconditional score difference map
3. Enhancement ê³¼ì • ì‹œê°í™” (spectrogram)
4. Failure case ë¶„ì„
```

---

### 0.3 Related Work ì •ë¦¬ (ì°¨ë³„í™” í¬ì¸íŠ¸)

#### ì§ì ‘ ê²½ìŸ ë…¼ë¬¸ë“¤

| Paper | Venue | Method | í•œê³„ì  | ìš°ë¦¬ì™€ì˜ ì°¨ì´ |
|-------|-------|--------|--------|--------------|
| **NASE** | Interspeech 2023 | Noise classification â†’ embedding | ì˜ëª»ëœ ë¶„ë¥˜ ì‹œ ì„±ëŠ¥ ì €í•˜ | CFGë¡œ unconditional fallback |
| **NADiffuSE** | ASRU 2023 | Direct noise encoding | OOD ëª…ì‹œì  ì²˜ë¦¬ ì—†ìŒ | CFG dropoutìœ¼ë¡œ OOD robustness |
| **N-HANS** | 2021 | Auxiliary sub-networks | Task-specific í•™ìŠµ í•„ìš” | End-to-end joint training |

#### ìš°ë¦¬ì˜ ì°¨ë³„ì  (Novelty)
1. **CFG ê¸°ë°˜ OOD Robustness**: ê¸°ì¡´ ë°©ë²•ë“¤ì€ conditioningì´ ì •í™•í•˜ë‹¤ê³  ê°€ì •. ìš°ë¦¬ëŠ” CFGë¡œ unreliable conditioningì— ëŒ€í•œ graceful degradation ì œê³µ
2. **Classification ë¶ˆí•„ìš”**: NASEì™€ ë‹¬ë¦¬ discrete noise class ì—†ì´ continuous embedding ì§ì ‘ ì‚¬ìš©
3. **Inference-time ìœ ì—°ì„±**: Guidance scale wë¡œ conditioning ê°•ë„ ì¡°ì ˆ ê°€ëŠ¥

#### References
- NASE: https://arxiv.org/abs/2307.08029
- NADiffuSE: https://arxiv.org/abs/2309.01212

---

### 0.4 ë…¼ë¬¸ Contribution ê°•í™” ë°©ì•ˆ

í˜„ì¬ contributionì´ ì•½í•¨. ë‹¤ìŒ ì¤‘ 1-2ê°œ ì¶”ê°€ í•„ìš”:

| ì¶”ê°€ Contribution | ë‚œì´ë„ | Impact | ì„¤ëª… |
|------------------|--------|--------|------|
| **Noise-type adaptive guidance** | Medium | High | Noise typeì— ë”°ë¼ guidance scale ìë™ ì¡°ì ˆ |
| **Self-supervised noise encoder** | High | High | Contrastive learningìœ¼ë¡œ noise encoder ì‚¬ì „í•™ìŠµ |
| **Lightweight noise encoder** | Low | Medium | íš¨ìœ¨ì ì¸ encoderë¡œ ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥ |
| **Multi-condition fusion** | Medium | Medium | SNR + noise type ë™ì‹œ conditioning |
| **Theoretical analysis** | High | High | CFGê°€ ì™œ OODì— ë„ì›€ë˜ëŠ”ì§€ ì´ë¡ ì  ë¶„ì„ |

---

## 1. Problem Statement

### 1.1 í˜„ì¬ ìƒí™©
Noise-conditioned SGMSE+ with CFGê°€ ê¸°ëŒ€í•œ ì„±ëŠ¥ ê°œì„ ì„ ë³´ì´ì§€ ì•ŠìŒ.

### 1.2 ê¸°ëŒ€ vs ì‹¤ì œ ê²°ê³¼

| Model | Dataset | Expected PESQ | Actual PESQ | Expected SI-SDR | Actual SI-SDR |
|-------|---------|---------------|-------------|-----------------|---------------|
| SGMSE+ baseline | VB-DEMAND | ~2.9 (ë…¼ë¬¸) | 1.88 | ~17 (ë…¼ë¬¸) | 13.1 |
| SGMSE+ baseline | OOD (ESC-50) | - | 1.17 | - | -0.2 |
| CFG p=0.2 | VB-DEMAND | â‰¥ baseline | 1.75 | â‰¥ baseline | 11.8 |
| CFG p=0.2 | OOD (ESC-50) | > baseline | 1.17 | > baseline | -0.6 |

### 1.3 í•µì‹¬ ë¬¸ì œ
1. **Baseline ì„±ëŠ¥ gap**: ë…¼ë¬¸ PESQ 2.9 vs ìš°ë¦¬ 1.88 (1.0 ì°¨ì´)
2. **CFGê°€ ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜**: In-distì—ì„œ baselineë³´ë‹¤ ë‚˜ì¨ (1.88 â†’ 1.75)
3. **OOD ê°œì„  ì—†ìŒ**: CFGê°€ OODì—ì„œë„ baselineë³´ë‹¤ ë‚˜ì¨ (-0.2 â†’ -0.6)
4. **PoCì™€ Scaled ê²°ê³¼ ë¶ˆì¼ì¹˜**: PoCì—ì„œëŠ” CFGê°€ OOD +1.4dB ê°œì„ ì´ì—ˆìŒ

---

## 2. ê°€ì„¤ ë° ë¶„ì„

### 2.1 Hypothesis A: Evaluation ì„¤ì • ë¬¸ì œ

**ì¦ìƒ**: Baseline ì„±ëŠ¥ì´ ë…¼ë¬¸ ëŒ€ë¹„ ë‚®ìŒ

**ê°€ëŠ¥í•œ ì›ì¸**:
- [ ] N (sampling steps) ë¶€ì¡±: 30 vs ë…¼ë¬¸ 50?
- [ ] Corrector ì„¤ì • ì°¨ì´
- [ ] EMA weights ë¯¸ì ìš©
- [ ] ë‹¤ë¥¸ evaluation í”„ë¡œí† ì½œ

**ê²€ì¦ ë°©ë²•**:
```bash
# 1. N=50ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
python scripts/eval_batch.py --phase all --gpus 4,5,6,7  # N_STEPS=50

# 2. ë…¼ë¬¸ pretrained checkpointë¡œ ìš°ë¦¬ eval ì½”ë“œ ê²€ì¦
# ë…¼ë¬¸ ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ í›„ ë™ì¼ eval ì‹¤í–‰

# 3. EMA ë¡œë”© í™•ì¸
python -c "
import torch
ckpt = torch.load('./logs/55jxu1gw/last.ckpt', map_location='cpu')
print('EMA in checkpoint:', 'ema' in ckpt)
if 'ema' in ckpt:
    print('EMA keys:', ckpt['ema'].keys())
"
```

---

### 2.2 Hypothesis B: Noise Encoder ë¬¸ì œ

**ì¦ìƒ**: CFG ëª¨ë¸ì´ baselineë³´ë‹¤ ì„±ëŠ¥ ì €í•˜

**ê°€ëŠ¥í•œ ì›ì¸**:
- [ ] Noise embeddingì´ ìœ ìš©í•œ ì •ë³´ë¥¼ ë‹´ì§€ ëª»í•¨
- [ ] Noise encoderê°€ underfitting
- [ ] Conditioning injection ë°©ì‹ ë¬¸ì œ (FiLM vs cross-attention)
- [ ] Noise reference ê¸¸ì´/í’ˆì§ˆ ë¬¸ì œ

**ê²€ì¦ ë°©ë²•**:
```python
# 1. Noise embedding ë¶„ì„
# - ê°™ì€ noise typeì˜ embeddingì´ clusterë¥¼ í˜•ì„±í•˜ëŠ”ì§€?
# - t-SNE/UMAPìœ¼ë¡œ ì‹œê°í™”

# 2. Noise encoder output í™•ì¸
# - Embeddingì´ collapse ë˜ì§€ ì•Šì•˜ëŠ”ì§€ (ëª¨ë‘ ë¹„ìŠ·í•œ ê°’?)
# - Embeddingì˜ variance í™•ì¸

# 3. Ablation: Noise embeddingì„ zeroë¡œ ê³ ì •í•˜ê³  í…ŒìŠ¤íŠ¸
# - ì„±ëŠ¥ ì°¨ì´ ì—†ìœ¼ë©´ conditioningì´ ë¬´ì‹œë˜ê³  ìˆëŠ” ê²ƒ
```

**ì½”ë“œ**:
```python
# embedding_analysis.py
import torch
from sgmse.model_cond import NoiseCondScoreModel

model = NoiseCondScoreModel.load_from_checkpoint(ckpt_path)
model.eval()

# ì—¬ëŸ¬ noise sampleì˜ embedding ì¶”ì¶œ
embeddings = []
for noise_sample in noise_samples:
    z_r = model.noise_encoder(noise_sample)
    embeddings.append(z_r.detach().cpu())

# ë¶„ì„
embeddings = torch.stack(embeddings)
print(f"Embedding mean: {embeddings.mean():.4f}")
print(f"Embedding std: {embeddings.std():.4f}")
print(f"Embedding norm range: [{embeddings.norm(dim=-1).min():.4f}, {embeddings.norm(dim=-1).max():.4f}]")
```

---

### 2.3 Hypothesis C: CFG Training ë¬¸ì œ

**ì¦ìƒ**: CFG p=0.2ê°€ PoCì—ì„œëŠ” íš¨ê³¼ ìˆì—ˆìœ¼ë‚˜ scaledì—ì„œëŠ” ì—†ìŒ

**ê°€ëŠ¥í•œ ì›ì¸**:
- [ ] Scaled trainingì—ì„œ CFG dropoutì´ ë‹¤ë¥´ê²Œ ë™ì‘
- [ ] Multi-GPU DDPì—ì„œ dropout ë™ê¸°í™” ë¬¸ì œ
- [ ] Batch size ì¦ê°€ë¡œ CFG íš¨ê³¼ ê°ì†Œ
- [ ] Learning rate scheduling ì°¨ì´

**ê²€ì¦ ë°©ë²•**:
```bash
# 1. PoC ì„¤ì •ìœ¼ë¡œ scaled ëª¨ë¸ ì¬í•™ìŠµ
# - 1 GPU, batch=4, 50k stepsë¡œ scaled checkpointì—ì„œ fine-tune

# 2. CFG dropout ì‹¤ì œ ë™ì‘ í™•ì¸
# - Training ì¤‘ ì‹¤ì œë¡œ conditioningì´ drop ë˜ëŠ”ì§€ ë¡œê¹…

# 3. Unconditional scoreì™€ conditional score ë¹„êµ
# - ì°¨ì´ê°€ ì—†ìœ¼ë©´ conditioningì´ í•™ìŠµ ì•ˆ ëœ ê²ƒ
```

---

### 2.4 Hypothesis D: Architecture ë¬¸ì œ

**ì¦ìƒ**: Noise conditioning ìì²´ê°€ íš¨ê³¼ ì—†ìŒ

**ê°€ëŠ¥í•œ ì›ì¸**:
- [ ] t_emb + noise_emb ë‹¨ìˆœ ë§ì…ˆì´ ë¹„íš¨ê³¼ì 
- [ ] Noise embedding dimension (512) ë¶€ì ì ˆ
- [ ] Score networkê°€ noise embeddingì„ ë¬´ì‹œí•˜ë„ë¡ í•™ìŠµë¨

**ê²€ì¦ ë°©ë²•**:
```python
# 1. Gradient flow í™•ì¸
# - Noise encoderë¡œ gradientê°€ íë¥´ëŠ”ì§€ í™•ì¸

# 2. Feature importance ë¶„ì„
# - Noise embeddingì„ perturbationí–ˆì„ ë•Œ output ë³€í™” ì¸¡ì •

# 3. ë‹¤ë¥¸ conditioning ë°©ì‹ í…ŒìŠ¤íŠ¸
# - Cross-attention
# - Adaptive normalization (AdaIN, AdaGN)
```

---

### 2.5 Hypothesis E: Data/Task ë¬¸ì œ

**ì¦ìƒ**: Noise conditioningì˜ ê·¼ë³¸ì  íš¨ìš© ì˜ë¬¸

**ê°€ëŠ¥í•œ ì›ì¸**:
- [ ] VB-DEMAND noiseê°€ ì´ë¯¸ ì¶©ë¶„íˆ ë‹¤ì–‘í•´ì„œ conditioning ë¶ˆí•„ìš”
- [ ] Noise referenceì—ì„œ ì¶”ì¶œ ê°€ëŠ¥í•œ ì •ë³´ê°€ noisy inputì— ì´ë¯¸ ìˆìŒ
- [ ] Oracle noise reference ì„¤ì •ì´ í˜„ì‹¤ì ì´ì§€ ì•ŠìŒ

**ê²€ì¦ ë°©ë²•**:
```bash
# 1. Random noise referenceë¡œ í…ŒìŠ¤íŠ¸
# - ì„±ëŠ¥ ì°¨ì´ ì—†ìœ¼ë©´ conditioningì´ ì‹¤ì œë¡œ í™œìš©ë˜ì§€ ì•ŠëŠ” ê²ƒ

# 2. ì™„ì „íˆ ë‹¤ë¥¸ noise referenceë¡œ í…ŒìŠ¤íŠ¸
# - ì„±ëŠ¥ í•˜ë½ ì—†ìœ¼ë©´ conditioning ë¬´ì‹œë˜ëŠ” ê²ƒ

# 3. Noise typeë³„ ì„±ëŠ¥ ë¶„ì„
# - íŠ¹ì • noise typeì—ì„œë§Œ íš¨ê³¼ ìˆëŠ”ì§€ í™•ì¸
```

---

## 3. Debugging Priority

### Phase 1: Evaluation ê²€ì¦ (ìµœìš°ì„ )
1. [TODO] N=50ìœ¼ë¡œ ì¬í‰ê°€
2. [TODO] ë…¼ë¬¸ pretrained checkpointë¡œ eval ì½”ë“œ ê²€ì¦
3. [TODO] EMA ë¡œë”© ìƒíƒœ í™•ì¸

### Phase 2: Conditioning íš¨ê³¼ ê²€ì¦
4. [TODO] Zero embedding vs real embedding ë¹„êµ
5. [TODO] Random noise reference í…ŒìŠ¤íŠ¸
6. [TODO] Noise embedding ì‹œê°í™” (t-SNE)

### Phase 3: Architecture ë¶„ì„
7. [TODO] Gradient flow í™•ì¸
8. [TODO] Conditional vs unconditional score ì°¨ì´ ë¶„ì„

### Phase 4: ëŒ€ì•ˆ íƒìƒ‰
9. [TODO] Cross-attention conditioning
10. [TODO] Stronger noise encoder (larger, pretrained)

---

## 4. Action Items

### ì¦‰ì‹œ ì‹¤í–‰
```bash
# 1. N=50 í‰ê°€ (ì§„í–‰ ì¤‘)
python scripts/eval_batch.py --phase all --gpus 4,5,6,7

# 2. ë…¼ë¬¸ pretrained checkpoint í…ŒìŠ¤íŠ¸ (ì§„í–‰ ì¤‘)
# Enhancement (ì§„í–‰ ì¤‘):
python enhancement.py --test_dir ./data/voicebank-demand/test/noisy --enhanced_dir ./enhanced_pretrained --ckpt pretrained_vbdmd.ckpt --N 50

# Metrics (enhance ì™„ë£Œ í›„ ì‹¤í–‰):
python calc_metrics.py --clean_dir ./data/voicebank-demand/test/clean --noisy_dir ./data/voicebank-demand/test/noisy --enhanced_dir ./enhanced_pretrained
```

### ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± í•„ìš”
- [ ] `scripts/analyze_embeddings.py`: Noise embedding ë¶„ì„
- [ ] `scripts/test_conditioning.py`: Conditioning íš¨ê³¼ í…ŒìŠ¤íŠ¸
- [ ] `scripts/compare_scores.py`: Conditional vs unconditional score ë¹„êµ

---

## 5. ì‹¤í—˜ ë¡œê·¸

### 2026-02-05: Initial Debug

**N=30 ê²°ê³¼**:
| Model | In-dist PESQ | In-dist SI-SDR | OOD PESQ | OOD SI-SDR |
|-------|--------------|----------------|----------|------------|
| sgmse_scaled | 1.88 | 13.1 | 1.17 | -0.2 |
| cfg_p0.2_scaled | 1.75 | 11.8 | 1.17 | -0.6 |

**N=50 ê²°ê³¼**: (ëŒ€ê¸° ì¤‘)
| Model | In-dist PESQ | In-dist SI-SDR | OOD PESQ | OOD SI-SDR |
|-------|--------------|----------------|----------|------------|
| sgmse_scaled | TBD | TBD | TBD | TBD |
| cfg_p0.2_scaled | TBD | TBD | TBD | TBD |

---

## 6. ì°¸ê³  ìë£Œ

### ê´€ë ¨ ë¬¸ì„œ
- `docs/EXPERIMENT_REPORT.md`: ì „ì²´ ì‹¤í—˜ ê²°ê³¼
- `docs/NOISE_COND_IMPROVEMENTS.md`: ë°©ë²•ë¡  ìƒì„¸

### ê´€ë ¨ ì½”ë“œ
- `sgmse/model_cond.py`: Noise-conditioned model
- `sgmse/backbones/ncsnpp_v2_cond.py`: Conditioned backbone
- `enhancement_noise_cond.py`: Noise-conditioned enhancement

---

## 7. Paper Improvement Roadmap

### Phase 1: ê¸°ìˆ ì  ë¬¸ì œ í•´ê²° (í˜„ì¬)
- [ ] Baseline ì„±ëŠ¥ gap ì›ì¸ íŒŒì•…
- [ ] CFG ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ ì›ì¸ íŒŒì•…
- [ ] Evaluation ì½”ë“œ ê²€ì¦

### Phase 2: í•µì‹¬ ì‹¤í—˜ ë³´ì™„
- [ ] Conditioning íš¨ê³¼ ì¦ëª… (zero vs real embedding)
- [ ] ì¶”ê°€ OOD ë°ì´í„°ì…‹ (UrbanSound8K)
- [ ] ë¹„êµ ì‹¤í—˜ (MetricGAN+ ë“±)

### Phase 3: Analysis ê°•í™”
- [ ] Noise embedding ì‹œê°í™” (t-SNE)
- [ ] Score difference ë¶„ì„
- [ ] Failure case ë¶„ì„

### Phase 4: ë…¼ë¬¸ ì‘ì„± ì™„ë£Œ
- [ ] Abstract ì‘ì„±
- [ ] Results í…Œì´ë¸” ì—…ë°ì´íŠ¸
- [ ] Conclusion ì‘ì„±
- [ ] Architecture Figure ì¶”ê°€

---

## 8. Quick Reference: í•µì‹¬ ì§ˆë¬¸ë“¤

ë…¼ë¬¸ acceptë¥¼ ìœ„í•´ ë‹µí•´ì•¼ í•  ì§ˆë¬¸ë“¤:

1. **Why noise conditioning?**
   - Noisy inputì— ì´ë¯¸ noise ì •ë³´ê°€ ìˆëŠ”ë° ì™œ ë³„ë„ referenceê°€ í•„ìš”í•œê°€?
   - â†’ ë‹µ: Explicit conditioningìœ¼ë¡œ ë” ì •í™•í•œ noise characterization ê°€ëŠ¥

2. **Why CFG?**
   - ë‹¨ìˆœ conditioning ëŒ€ì‹  CFGë¥¼ ì“°ëŠ” ì´ìœ ëŠ”?
   - â†’ ë‹µ: OOD noiseì—ì„œ graceful degradation, unconditional fallback

3. **What does noise encoder learn?**
   - Encoderê°€ ì˜ë¯¸ìˆëŠ” noise representationì„ í•™ìŠµí•˜ëŠ”ê°€?
   - â†’ ë‹µ: t-SNEë¡œ noise type clustering ì‹œê°í™” í•„ìš”

4. **When does it fail?**
   - ì–´ë–¤ ìƒí™©ì—ì„œ baselineë³´ë‹¤ ë‚˜ë¹ ì§€ëŠ”ê°€?
   - â†’ ë‹µ: Failure case ë¶„ì„ í•„ìš”

5. **Is it practical?**
   - ì‹¤ì œ í™˜ê²½ì—ì„œ noise referenceë¥¼ ì–´ë–»ê²Œ ì–»ëŠ”ê°€?
   - â†’ ë‹µ: Voice activity detectionìœ¼ë¡œ noise-only êµ¬ê°„ ì¶”ì¶œ

---

## 9. Experiment Checklist for Submission

### Must Have (Accept í•„ìˆ˜ì¡°ê±´)
- [ ] Baseline ì„±ëŠ¥ ì¬í˜„ (PESQ > 2.5)
- [ ] CFGê°€ OODì—ì„œ baseline ëŒ€ë¹„ ê°œì„ 
- [ ] Conditioning íš¨ê³¼ ì¦ëª… ì‹¤í—˜
- [ ] ìµœì†Œ 2ê°œ OOD ë°ì´í„°ì…‹

### Should Have (ê²½ìŸë ¥ í™•ë³´)
- [ ] 1ê°œ ì´ìƒ ë¹„êµ ë°©ë²•
- [ ] Noise embedding ì‹œê°í™”
- [ ] Ablation study (p, w, ref_length)

### Nice to Have (ê°•ë ¥í•œ ë…¼ë¬¸)
- [ ] 3ê°œ ì´ìƒ ë¹„êµ ë°©ë²•
- [ ] Real-world evaluation
- [ ] ì¶”ê°€ contribution (adaptive guidance ë“±)
- [ ] Theoretical analysis

---

*Created: 2026-02-05*
*Last Updated: 2026-02-05*
*Status: Active Investigation - Phase 1*
